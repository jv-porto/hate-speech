{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código com base no repositório [Bert_HateSpeech_Classification](https://github.com/cewebbr/Bert_HateSpeech_Classification) de Diogo Cortiz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar o Pytorch e HuggingFace (transformers e tokenizers)\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertForMaskedLM, PreTrainedTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"OPCIONAL: Pode ativar um debbug para entender melhor o que acontece\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processando o Dataset\n",
    "\n",
    "Na pasta, existem duas pastas. Precisamos usar os dados brutos (raw) para que possamos criar os token usando o BertTokenizer. Não podemos usar as setenças tokenizadas porque o processo utilizado na criação desses tokens foi diferente do usado pelo Bert.\n",
    "\n",
    "Iremos utilizar o pandas para manipular as entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hatespeech_comb</th>\n",
       "      <th>hatespeech_G1</th>\n",
       "      <th>annotator_G1</th>\n",
       "      <th>hatespeech_G2</th>\n",
       "      <th>annotator_G2</th>\n",
       "      <th>hatespeech_G3</th>\n",
       "      <th>annotator_G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@__andrea__b \\nO cara vive em outro mundo\\nNão...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@_carmeloneto \\nOs 'cumpanhero' quebraram toda...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@_GlitteryKisses é isso não conseguem pensar n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_iglira bom dia macaco branco haha</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hatespeech_comb  \\\n",
       "0  @__andrea__b \\nO cara vive em outro mundo\\nNão...                1   \n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...                0   \n",
       "2  @_carmeloneto \\nOs 'cumpanhero' quebraram toda...                0   \n",
       "3  @_GlitteryKisses é isso não conseguem pensar n...                0   \n",
       "4                @_iglira bom dia macaco branco haha                1   \n",
       "\n",
       "   hatespeech_G1 annotator_G1  hatespeech_G2 annotator_G2  hatespeech_G3  \\\n",
       "0              1            A            1.0            V              0   \n",
       "1              1            D            0.0            V              0   \n",
       "2              1            A            0.0            B              0   \n",
       "3              0            C            0.0            V              0   \n",
       "4              0            A            1.0            I              1   \n",
       "\n",
       "  annotator_G3  \n",
       "0            E  \n",
       "1            C  \n",
       "2            E  \n",
       "3            D  \n",
       "4            E  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/2019-05-28_portuguese_hate_speech_binary_classification.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto, o dataset possui diversos campos de registro das anotações. <br> O que nos interessa, no entanto, são os campos do texto \"text\" e a classificação final \"hatespeech_comb\". \n",
    "\n",
    "Podemos criar dos arrays (Numpy Array) para armazenar as setenças e os labels. Também vamos deixar todos os textos em letras minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# DataFrame.value retorna uma representação Numpy\n",
    "sentencas = df['text'].values\n",
    "labels = df['hatespeech_comb'].values\n",
    "\n",
    "# Transformando as setenças em letras minúsculas\n",
    "sentencas = [sentenca.lower() for sentenca in sentencas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatando a Entrada e Criando os Tokens e IDS\n",
    "\n",
    "Agora chegou a hora de criarmos os tokens para que depois eles possam ser mapeados para os index do vocabulário.<Br>\n",
    "Para isso, utilizaremos o BertTokenizer, que já importamos anteriormente. Utilizaremos a versão 'bert-based-unscased' (inglês).<br>\n",
    "Antes disso, no entanto, precisamos formatar a entrada para respeitar os requisitos de entrada do Bert, que são os seguintes:\n",
    "\n",
    "\n",
    "1.   Adicionar tokens especiais no início e no fim de cada sentença ([CLS] para o início de uma sentença de classificação / [SEP] para indicar o fim de uma sentença.\n",
    "\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png\" alt=\"placeholder\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "2. Precisamos fazer o Pad e Truncate para que todas as senteças tenham o mesmo tamanho de entrada (máximo de 512 tokens).\n",
    "\n",
    "3. Diferenciar o que são tokens reais dos token de padding, criando um \"attention mask\". \n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" alt=\"placeholder\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "## Entendendo o Tokenizer<br>\n",
    "O modelo do Google tem um vocabulário de 30.000 tokens com aprox 768 dimensões cada (embeedings).<br>\n",
    "Os Tokens podem representar palavras completas (mas não todas) <br>\n",
    "Os Tokens podem representar sub-palavras de palavras desconhecidas (as subwords são iniciadas com ##. Reparem que a palavra Diogo não existe no vocabulário. Ela é quebrada em duas subwords: Dio + ##go. Atenção: O token go é diferente do ##go) <oov><br>\n",
    "Os Tokens podem representar caracteres e marcações especiais:<br>\n",
    "[PAD]: Padding<br>\n",
    "[UNK]:Unknow<br>\n",
    "[CLS]: Classificação<br>\n",
    "[SEP]: Separação das sentenças<br>\n",
    "[MASK]: Máscara para a palavra<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "# USA O MODULO TOKENIZER DO HUGGINFACE\n",
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                            SentencePieceBPETokenizer,\n",
    "                            BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5670\n",
      "5670\n",
      "Encoding(num_tokens=60, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['[CLS]', '@', '_', '_', 'and', '##rea', '_', '_', 'b', 'o', 'cara', 'vive', 'em', 'outro', 'mundo', 'não', 'no', 'mundo', 'real', 'refugiados', 'são', 'os', 'que', 'vivem', 'nas', 'fa', '##velas', 'vizinhas', 'as', 'suas', 'fortaleza', '##s', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# CRIA O TOKENIZER A PARTIR DE UM VOCABULÁRIO\n",
    "# LOWERCASE = FALSE (NÃO IRÁ CONVERTER AS ENTRADAS PARA LOWERCASE. MANTEM O ORGINIAL)\n",
    "# STRIP ACCENTS = FALSE (MANTEM OS ACENTOS)\n",
    "tokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=False, strip_accents=False)\n",
    "\n",
    "# PERMITE O TRUNCATION E O PADDING\n",
    "tokenizer.enable_truncation(max_length=60)\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "# TOKENINZA EM BATCH TODAS AS SENTENÇAS\n",
    "# TEM QUE USAR .TOLIST PARA CONVERTER POR LISTA. SENTENCAS É UM ARRAY NUMPY\n",
    "output = tokenizer.encode_batch(sentencas)\n",
    "\n",
    "# O TOKENIZER RETORAR UMA LISTA DE OBJETOS DO TIPO TOKENIZER\n",
    "# PRECISAMOS PEGAR OS ATRIBUTOS IDS E MASKS E ADICIONAR PARA LISTAS\n",
    "# OS OBJETOS TEM O ATRIBUTO IDS(IDS), TOKENS (TOKENS) E attention_mask\n",
    "# PRECISAMOS FAXER O FOR PARA PEGAR CADA UM E DEPOIS CRIAR A LISTA\n",
    "ids=[x.ids for x in output]\n",
    "attention_mask = [x.attention_mask for x in output]\n",
    "\n",
    "print(len(ids))\n",
    "print(len(attention_mask))\n",
    "\n",
    "# PRINTS EXEMPLO DE SAIDA DA PRIMEIRA LINHA\n",
    "print(output[0])\n",
    "print(output[0].tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo o Dataset em Treinamento e Validação\n",
    "\n",
    "Vamos usar a ferramenta do ScikitLearn para nos ajudar neste processo. Vamos dividir o dataset em 80% para treinamento e 20% para a validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6434, 11842, 698, 8094, 100, 10161, 127, 4522, 3306, 4863, 1141, 106, 291, 117, 271, 13105, 22278, 137, 528, 1927, 6288, 8393, 1604, 117, 253, 1203, 16188, 159, 123, 651, 106, 108, 139, 22298, 22285, 108, 734, 1111, 451, 6702, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#USAR O MESMO RANDON_STATE PARA NAO TROCAR OS INPUTS DE SUAS MÁSCARAS\n",
    "train_input, validation_input, train_labels, validation_labels = train_test_split(ids, labels, random_state=2018, test_size=0.2)\n",
    "train_mask, validation_mask, _, _ = train_test_split(attention_mask, labels, random_state=2018, test_size=0.2)\n",
    "\n",
    "#COMPARANDO A PRIMEIRA LINHA DE TREINAMENTO COM A MASCARA\n",
    "print(train_input[0])\n",
    "print(train_mask[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os tensores (Pytorch Data Type)\n",
    "Os modelos do PyTorch esperam de entrada o tipo tensor, então precisamos converter o nosso dataset de Numpy Array para tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_tensor = torch.tensor(train_input)\n",
    "validation_input_tensor = torch.tensor(validation_input)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "validation_labels_tensor = torch.tensor(validation_labels)\n",
    "\n",
    "train_mask_tensor = torch.tensor(train_mask)\n",
    "validation_mask_tensor= torch.tensor(validation_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma ação adicional é usar o torch DataLoader, que cria um \"iterator\". Diferente de um for, o iterador não sobe todo o dataset não precisa ser carregado todo na memória (ajuda no treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#É PRECISO ESPECIFICAR O TAMANHO DO BATCH, PARA O BERT OS AUTORES RECOMENDAM 16 OU 32\n",
    "batch_size = 32\n",
    "\n",
    "#CRIA OS DATALOADERS PARA O CONJUNTO DE TREINAMENTO\n",
    "train_data = TensorDataset(train_input_tensor, train_mask_tensor, train_labels_tensor)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#CRIA OS DATALOADRES PARA O CONJUNTO DE VALIDAÇÃO\n",
    "validation_data = TensorDataset(validation_input_tensor, validation_mask_tensor, validation_labels_tensor)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo\n",
    "O Bert oferece um modelo pré-treinando a qual só precisamos fazer fine-tune para a tarefas que desejamos. O huggingface disponibiza não só o modelo pré-treinado mas também interfaces para nossas tarefas específicas. Algumas disponíveis são:\n",
    "\n",
    "\n",
    "*   BertMode\n",
    "*   BertForMaskedLM\n",
    "*   BertForNextSentencePrediction\n",
    "*   BertForSequenceClassification (vamos usar este)\n",
    "*   BertForTokenClassification\n",
    "*   BertForQuestionAnsering\n",
    "\n",
    "O BertForSequenceClassification basicamente é a implementação do modelo Bert com a adição de uma camada de FFN para classificação. Lembre-se que o hugging face disponibilizou diversas versões de modelo pré-treinanda (base, large, multilanugage). Você pode escolher a que for melhor para o seu propósito. Neste caso, vamos utilizar a versão multilingual por contemplar o português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#IMPORTA O BERT E O OTIMIZADOR ADAM\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertModel\n",
    "\n",
    "#CRIA O MODELO BERT PRETREINADO COM UMA CAMADA DE CLASSIFICAÇÃO NO TOPO\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'neuralmind/bert-base-portuguese-cased',\n",
    "    num_labels = 2, # NUMERO DE CLASSES (NO CASO BINÁRIA: ACEITÁVEL OU NÃO. PODE TER MAIS PARA MULTICLASSE)\n",
    "    output_attentions = True, # SE O MODELO DEVE EXPORTAR OS PESOS DAS ATENÇÕES\n",
    "    output_hidden_states = True, # SE O MODELO DEVE EXPORTAR OS HIDDEN STATES (PODE SER INTERESSANTE PARA ESTUDAR EMBEDDINGS)\n",
    ")\n",
    "\n",
    "#DIZ AO MODELO PARA USAR GPU\n",
    "# model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exibindo parâmetros do modelo\n",
    "Uma curiosidade disponibilizada em (https://mccormickml.com/2019/07/22/BERT-fine-tuning/) <br>\n",
    "É possível mostrar os parâmetro do modelo:\n",
    "<br>\n",
    "*   A camada de embeddings\n",
    "*   A primeira das 12 camadas de transformers\n",
    "*   A camada de saída (output)\n",
    "\n",
    "A execução do trecho abaixo é optativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (29794, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizador\n",
    "\n",
    "Carregamos o modelo, agora precisamos criar o Otimizador Adam. Os autores recomendam os seguintres valores:\n",
    "*   Batch Size: 16, 32 (Lembre-se que usamos 32 no Dataloader)\n",
    "*   Learning Rate (Adam):  5e-5, 3e-5, 2e-5 (vamos usar 2e-5)\n",
    "*   Numero de épocas (Quantas vezes TODO o dataset é treinado):  2,3,4 (utilizaremos 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler\n",
    "Em redes neurais é útil diminuir a taxa de aprendizado (learning rate) conforme as épocas vão aumentando para que possamos evitar que o modelo entre em um estado \"caótico\" com taxas grandes ou o \"falso míninmo\" com taxas pequenas. A ideia é ir ajustando conforme as épocas vão passando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTA CLASSE FARÁ O AGENDAMENTO\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4 #QUANTIDADE DE ÉPOCAS\n",
    "\n",
    "#PARA CALCULAR A QUANTIDADE DE PASSOS É A QTD DE BATCHS * ÉPOCAS\n",
    "total_steps = epochs * len(train_dataloader)\n",
    "\n",
    "#CRIANDO O AGENDADOR\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, #VALOR PADRÃO\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de Treinamento\n",
    "Não é só chamar alguma função para treinar o modelo. Precisamos criar um loop que se repita a quantidade de épocas especificadas executando as atividades abaixo. A cada passagem, também faremos uma avaliação do modelo:\n",
    "\n",
    "Loop de treinamento\n",
    "*   Desempacotar os dados de entrada e os labels\n",
    "*   Carregar os dados para a GPU\n",
    "*   Limpar os gradientes calculados na passagem anterior (no pytorch os gradientes são acumulados por padrão. pode ser útil para RNN, mas não no caso de transformers.\n",
    "* Forward Pass (Passar os dados pela rede)\n",
    "* Backward Pass (backpropagation)\n",
    "* Pedir para a rede atualizar os parâmetros (optimizer.step())\n",
    "* Monitar as variáveis para saber o progresso\n",
    "\n",
    "Loop de avaliação\n",
    "* Desempacotar os dados de entrada e os labels\n",
    "* Carregar os dados para a GPU\n",
    "* Forward Pass (Passar os dados pela rede)\n",
    "* Computar a perda na nossa validação e monitorar as variáveis para saber o progresso.\n",
    "\n",
    "**Antes, vamos criar duas funções de ajuda. Uma para calcular a acurácia do modelo e outra para formatar o horário**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# FUNÇÃO  QUE CALCULA A ACURÁCIA DO MODELO (PREDIÇÕES vs LABELS)\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# FUNÇÃO QUE FORMATA O HORÁRIO\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"**AGORA VEM O LOOP DE TREINAMENTO :)**\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "# PRIMEIRO PRECISAMOS GARANTIR A REPRODUTIBILIDADE\n",
    "# USANDO OS SEEDS DO PYTORCH, GARANTIMOS QUE OS VALORES SERÃO INICIADOS DA MESMA FORMA\n",
    "# VAMOS SETAR O MESMO VALOR EM DIFERENTES LUGARES\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# CRIANDO UMA LISTA QUE IRÁ ARMZENAR LOSS AO FIM DE CADA ÉPOCA\n",
    "loss_values = []\n",
    "\n",
    "# CRIANDO O LOOP DAS ÉPOCAS\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # MEDIR QUANTO TEMPO UMA ÉPOCA LEVA\n",
    "    t0 = time.time()\n",
    "\n",
    "    # RESETANDO O LOSS PARA ESTA ÉPOCA\n",
    "    total_loss = 0\n",
    "\n",
    "    #COLOCANDO O MODELO NO MODO DE TREINAMENTO\n",
    "    #ESSE COMANDO NÃO CHAMA O TREINAMENTO, APENAS AVISA O MODELO PARA FAZER AJUSTES DE DROPOUTS\n",
    "    model.train()\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # UM LOOP PARA CADA BATCH  DENTRO DA ÉPOCA\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # PRECISAMOS DESPEMPACOTAR O BATCH E CARREGAR NA GPU\n",
    "        # BATCH CONTEM TRÊS TENSORES\n",
    "        #   [0]: ID DE INPUT\n",
    "        #   [1]: ATTENTION MASKS\n",
    "        #   [2]: LABELS\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        #PRECISAMOS LIMPAR O GRADIENTE ANTES DE BACKPROP\n",
    "        #PYTORCH NAO FAZ ISSO AUTOMÁTICO\n",
    "        model.zero_grad()\n",
    "\n",
    "        #AGORA VAMOS FAZER UMA PASSAGEM (FORWARD PASS)\n",
    "        #O RESULTADO SERÁ LOSS (NÃO SERÁ A PREDIÇÃO PQ PASSAMOS OS LABELS)\n",
    "        outputs = model(b_input_ids,\n",
    "                    token_type_ids=None, #USADO QUANDO É NEXT SEQUENCE\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels)\n",
    "\n",
    "        #O MODELO RETORNA UMA TUPLA.\n",
    "        #VAMOS PEGAR O VALOR DA TUPLA\n",
    "        loss = outputs[0]\n",
    "        hidden_state = outputs[2]\n",
    "\n",
    "\n",
    "        # VAMOS ARMAZENAR O HIDDEN STATE E ATENÇÃO TB\n",
    "\n",
    "\n",
    "\n",
    "        #VAMOS ACUMULAR O VALOR NO TOTAL DE LOSS DA ÉPOCA\n",
    "        # .item() RETORNA UM VALOR PYTHON DE UM TENSOR\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #AGORA VAMOS FAZER O BACKWARD PARA CALCULAR O GRADIENTE\n",
    "        loss.backward()\n",
    "\n",
    "        # PASSO NECESSÁRIO\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # O OTIMIZADOR VAI ATUALIZAR OS PARAMETROS COM BASE NO GRADIENTE\n",
    "        optimizer.step()\n",
    "\n",
    "        # ATUALIZANDO O LEARNING RATE\n",
    "        scheduler.step()\n",
    "\n",
    "    # APOS TODOS OS BATCH DE UMA EPOCA\n",
    "    # CACLULA AVERAGE LOSS COM BASE NO TREINAMENTO (TAMANHO DO DATASET)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    #ARMAZENA O LOSS NA LISTA (PARA DEPOIS SER PLOTADO NO GRAFICO)\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    # DENTRO DE CADA ÉPOCA TAMBÉM VAMOS RODAR UMA AVALIAÇÃO\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # COLOCANDO O MODELO NO MODO DE AVALIAÇÃO (SAINDO DO MODULO DE TREINAMENTO)\n",
    "    model.eval()\n",
    "\n",
    "    # CRIANDO VARIÁVEIS DE MONITORAMENTO\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # LOOP PARA AVALIAR CADA BATCH DE TREINAMENTO\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # PEDE AO MODELO PARA NAO COMPUTAR GRADIENTES (É VALIDAÇÃO, NÃO TREINAMENTO)\n",
    "        with torch.no_grad():\n",
    "          # FORWARD PASS PARA CALCULAR OS LOGITS DA PREDIÇÃO\n",
    "          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        # O RESULTADO DO MODELO AGORA NÃO SERÁ LOSS\n",
    "        # SERÁ 'LOGITS', VALOR DE SAIDA ANTES DE UMA FUNÇÃO DE ATIVAÇÃO (SOFTMAX POR EXEMPLO)\n",
    "        # COMO É UMA CLASSIFICAÇÃO BINÁRIA, SÓ OS LOGITS SERVEM\n",
    "        # DEPENDENDO DO MODELO SERIA NECESSÁRIO UM SOFTMAX\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # MOVER OS LOGITS E OS LABELS PARA A CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        #CALCULAR ACURÁCIA CHAMANDO A FUNÇÃO QUE CRIAMOS ANTERIORMENTE\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # ACUMULAR O TOTAL DA ACURÁCIA\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # TRACKEAR O NUMERO DE BATCHS\n",
    "        nb_eval_steps +=1\n",
    "\n",
    "    # EXIBINDO DADOS FINAIS\n",
    "    print(\"  Acurácia: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Tempo de Validação: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "#FIM DAS EPOCAS\n",
    "print(\"FIM DO TREINAMENTO\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
